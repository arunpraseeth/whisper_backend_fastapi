ğŸ“¦ 1.1 Create a Python Virtual Environment
python3 -m venv venv
source venv/bin/activate

ğŸ“¦ 1.2 Install Required Packages

ğŸ§  1.3 FastAPI App Structure
backend/
â”‚
â”œâ”€â”€ main.py                 # FastAPI app
â”œâ”€â”€ whisper_utils.py        # Whisper functions
â”œâ”€â”€ bark_utils.py           # Bark functions
â”œâ”€â”€ models/
â”‚   â””â”€â”€ your LLM model

âœï¸ 1.4 Write FastAPI Code (main.py)

ğŸ”Š 1.5 Whisper Utility (whisper_utils.py)

ğŸ§  1.6 Connect to LLM

!!!!!!!!!! If you donot have GPU proceed to 1.8 (bark requires GPU if not it will be slow)!!!!!!!!!!

ğŸ”ˆ 1.7 Bark Utility (bark_utils.py) (In order to make the process faster - need a GPU)
    * This will download a 5GB file for the first time.
    * There will be a issue regarding weights_load:
        torch.load() now loads only weights for safety and we have to specify that in bark - generation.py file (making it safe)
        Under lib/python3.9/site-packages/bark/
            generation.py
                change: checkpoint = torch.load(ckpt_path, map_location=device) to checkpoint = torch.load(ckpt_path, map_location=device, 
                weights_only = False)
    * Again it will download few more files

    1.8 - Update - I do not have GPU and so it's taking more time to synthesize text to speech, so I'm using native TTS in my mobile.


ğŸ“± STEP 2: Connect Flutter to FastAPI
ğŸ—£ï¸ 2.1 Flutter UI Sends Voice